# Level 3: Finite Differences and the Heat Equation

::: {.dialogue}
**Pavni:** Acharya, how do we actually *approximate* derivatives on a computer?

**Acharya:** We use the **finite difference method**. A computer only works with discrete points, so we replace continuous derivatives with **difference quotients** on a **grid**. These come directly from the **Taylor series**, and because we truncate the series, each formula comes with a predictable **error term**.

**Pavni:** Can you give me an example?

**Acharya:** Suppose we have points $x_0, x_1, \dots, x_N$ with spacing $\Delta x$. Around a point $x_i$, Taylor’s theorem gives us expansions for $u(x_{i+1})$ and $u(x_{i-1})$. By combining them, we obtain difference formulas:

- **Forward difference** for the first derivative:
$$
u'(x_i) = \frac{u(x_{i+1}) - u(x_i)}{\Delta x} - \tfrac{1}{2}u''(\xi)\,\Delta x,
$$
so the truncation error is $\mathcal{O}(\Delta x)$.

- **Backward difference**:
$$
u'(x_i) = \frac{u(x_i) - u(x_{i-1})}{\Delta x} + \tfrac{1}{2}u''(\xi)\,\Delta x,
$$
also error $\mathcal{O}(\Delta x)$.

- **Central difference**:
$$
u'(x_i) = \frac{u(x_{i+1}) - u(x_{i-1})}{2\Delta x} - \tfrac{1}{6}u^{(3)}(\xi)\,(\Delta x)^2,
$$
so the error is $\mathcal{O}((\Delta x)^2)$ — more accurate.

- **Second derivative** (central difference):
$$
u''(x_i) = \frac{u(x_{i-1}) - 2u(x_i) + u(x_{i+1})}{(\Delta x)^2} - \tfrac{1}{12}u^{(4)}(\xi)\,(\Delta x)^2,
$$
with error $\mathcal{O}((\Delta x)^2)$.

**Pavni:** So these formulas are not exact — they always come with an error term?

**Acharya:** Precisely. That error is called the **local truncation error**. When we build numerical schemes for PDEs, these errors accumulate step by step, and we’ll have to balance them with stability.  

**Pavni:** I see. So finite differences give us algebraic formulas for derivatives, but with a known accuracy.

**Acharya:** Exactly. That’s the foundation of finite difference methods.
:::


## Application: The Heat Equation

::: {.dialogue}
**Acharya:** Consider the 1D heat equation:
$$
u_t = \alpha u_{xx}^2, \quad 0 < x < 1,\; t>0
$$
with boundary conditions $u(0,t) = u(1,t) = 0$ and initial profile $u(x,0) = f(x)$.

We set up a grid:
- In space: $x_i = i\Delta x,\; i=0,\dots,N$  
- In time: $t^n = n\Delta t,\; n=0,1,2,\dots$

At each point, let $u_i^n \approx u(x_i,t^n)$.

**Pavni:** And now we replace derivatives?

**Acharya:** Correct.  
- Time derivative (forward difference):
  $$
  u_t(x_i,t^n) \approx \frac{u_i^{n+1} - u_i^n}{\Delta t}
  $$
- Spatial second derivative (central difference):
  $$
  u_{xx}(x_i,t^n) \approx \frac{u_{i-1}^n - 2u_i^n + u_{i+1}^n}{(\Delta x)^2}
  $$

Plugging these into the PDE, we get:
$$
\frac{u_i^{n+1} - u_i^n}{\Delta t} = \alpha^2 \,\frac{u_{i-1}^n - 2u_i^n + u_{i+1}^n}{(\Delta x)^2}.
$$

Rearranging:
$$
u_i^{n+1} = u_i^n + \lambda\,(u_{i-1}^n - 2u_i^n + u_{i+1}^n),
$$
where $\lambda = \frac{\alpha^2 \Delta t}{(\Delta x)^2}$.
:::

---

## Stability Condition

::: {.dialogue}
**Pavni:** So we can just keep applying this formula to march forward in time?

**Acharya:** Yes, but with a caveat. This scheme, called **FTCS (Forward Time, Central Space)**, is only stable if:
$$
\lambda \leq \tfrac{1}{2}.
$$

**Pavni:** So if $\Delta t$ is too large, the scheme fails?

**Acharya:** Exactly. The numerical solution will blow up, even though the true solution is stable. Choosing $\Delta t$ small enough ensures stability.
:::
::: {.callout-note collapse=true}
### Why the FTCS stability condition is $\lambda \leq \tfrac{1}{2}$

If the update is 
$$
u^{(n+1)} = A u^{(n)},
$$ 
and the initial error is $e^0$, then after $n$ steps the error is
$$
e^{(n)} = A^n e^0.
$$

Using an induced matrix norm:
$$
\|e^{(n)}\| = \|A^n e^0\| \;\le\; \|A^n\|\,\|e^0\|
\;\le\; \|A\|^n \|e^0\|.
$$

- If $\|A\|\le 1$, the error never grows.  
- If $\|A\|<1$, the error decays as $n\to\infty$.  
- In general, the asymptotic condition is that the **spectral radius** $\rho(A)<1$.

---

#### Eigenvalues of the FTCS matrix

For the FTCS tridiagonal matrix $A$ (symmetric Toeplitz), the eigenvalues are
$$
\mu_k = 1 - 4 \lambda \,\sin^2\!\left(\frac{k\pi}{2(N-1)}\right), 
\qquad k=1,2,\dots,N-2,
$$
where $\lambda = \dfrac{\alpha^2 \Delta t}{(\Delta x)^2}$.

Thus the spectral radius is
$$
\rho(A) = \max_{1\le k \le N-2} \big|\mu_k\big|
= \max_{1\le k \le N-2} \left|1 - 4r \sin^2\!\left(\tfrac{k\pi}{2(N-1)}\right)\right|.
$$

---

#### Stability condition

- As $N\to\infty$ (i.e. $\Delta x \to 0$), the maximum of $\sin^2(\cdot)$ tends to $1$.  
- Therefore the most restrictive case is
$$
|1 - 4 \lambda| \le 1.
$$
- This simplifies to
$$
0 \;\le\; \lambda \;\le\; \tfrac{1}{2}.
$$

---

✅ Thus, the **FTCS scheme is stable if and only if**
$$
\lambda = \frac{\alpha \Delta t}{(\Delta x)^2} \;\le\; \tfrac{1}{2}.
$$
:::

---

## Mini-Quizzes
::: {.quiz}
**Quiz 1:**  
Let $u(x) = x^2$. With $\Delta x = 0.1$, approximate $u'(1)$ using:  
1. Forward difference  
2. Central difference  

Compare with the exact derivative $u'(1) = 2$. Which is more accurate?
:::

::: {.callout-tip collapse=true}
### Answer 1

- Forward difference:
  $$
  \frac{u(1+\Delta x)-u(1)}{\Delta x}=\frac{(1.1)^2-1^2}{0.1}=\frac{1.21-1}{0.1}=2.1.
  $$

- Central difference:
  $$
  \frac{u(1+\Delta x)-u(1-\Delta x)}{2\Delta x}
  =\frac{(1.1)^2-(0.9)^2}{0.2}=\frac{1.21-0.81}{0.2}=2.0.
  $$

- Exact derivative: $u'(1)=2$.

**Conclusion:** The central difference gives the exact value here (error $0$), while the forward difference has error $0.1$. Central is more accurate (as expected — it is second-order).
:::

---

::: {.quiz}
**Quiz 2:**  
Suppose $\alpha = 1$, $\Delta x = 0.1$. What is the maximum $\Delta t$ for stability in the explicit scheme?  

(Hint: $\lambda = \dfrac{\alpha \Delta t}{(\Delta x)^2} \leq \tfrac{1}{2}$.)
:::

::: {.callout-tip collapse=true}
### Answer 2

We need
$$
\lambda=\frac{\alpha\,\Delta t}{(\Delta x)^2}\le\frac{1}{2}.
$$
With $\alpha=1$ and $\Delta x=0.1$, $(\Delta x)^2=0.01$. So
$$
\frac{\Delta t}{0.01}\le\frac{1}{2}\quad\Rightarrow\quad
\Delta t \le 0.01\times\frac{1}{2}=0.005.
$$

**Maximum allowable** $\displaystyle \Delta t = 0.005$.
:::

## Heat equation — Matrix Method with Non-zero Dirichlet Conditions

::: {.dialogue}
**Pavni:** We already saw how to discretize the heat equation with FTCS. But can we write the whole scheme in a more compact way?

**Acharya:** Yes. That’s where the **matrix method** comes in. Let’s recall the PDE:
$$
u_t = \alpha^2 u_{xx}, \quad 0 < x < 1, \; t > 0,
$$
with Dirichlet conditions
$$
u(0,t) = L, \qquad u(1,t) = R,
$$
and initial condition $u(x,0)=f(x)$.

**Pavni:** So we still set up the grid in space and time?

**Acharya:** Exactly. The FTCS update at interior nodes is
$$
u_i^{j+1} = u_i^j + r\,(u_{i-1}^j - 2u_i^j + u_{i+1}^j),
\quad i=1,\dots,N_x-2,
$$
where $\lambda = \tfrac{\alpha \Delta t}{\Delta x^2}$.

**Pavni:** That’s a lot of coupled equations. How do we collect them?

**Acharya:** We put all interior values into a vector
$$
u^{(j)} =
\begin{bmatrix}
u_1^j \\
u_2^j \\
\vdots \\
u_{N_x-2}^j
\end{bmatrix}.
$$

**Pavni:** And the update rule becomes a matrix multiplication?

**Acharya:** Yes. We can write
$$
u^{(j+1)} = A\,u^{(j)} + b,
$$
where $A$ is tridiagonal and $b$ accounts for the boundary conditions:
$$
A =
\begin{bmatrix}
1-2\lambda & \lambda     &        &        &   \\
r     & 1-2\lambda & \lambda      &        &   \\
      & \ddots & \ddots & \ddots &   \\
      &        & r      & 1-2\lambda   & \lambda \\
      &        &        & \lambda      & 1-2\lambda
\end{bmatrix}, \qquad
b =
\begin{bmatrix}
\lambda L \\
0 \\
\vdots \\
0 \\
\lambda R
\end{bmatrix}.
$$

**Pavni:** So if $L=R=0$, then $b=0$ and we just have $u^{(j+1)} = A u^{(j)}$.

**Acharya:** Precisely. That’s the beauty of the matrix method: it organizes the scheme into a linear algebra update.
:::

---

### Implementation in Python

Let us now implement the method in Python and compare with the exact solution for $f(x)=\sin(\pi x)$:
$$
u(x,t) = e^{-\pi^2 \alpha t}\,\sin(\pi x).
$$

```{python echo=false code-fold=true code-summary="Show code"}
import numpy as np
import matplotlib.pyplot as plt

alpha = 1.0
Nx    = 50
T     = 0.1
L, R  = 0.0, 0.0

dx = 1.0/(Nx-1)
dt_max = dx*dx/(2*alpha)
s  = 0.4
dt = s*dt_max
Nt = int(T/dt)
dt = T/Nt
r  = alpha*dt/dx**2

x  = np.linspace(0.0, 1.0, Nx)
u0 = np.sin(np.pi * x)

m = Nx - 2
main = (1 - 2*r) * np.ones(m)
off  = r * np.ones(m-1)
A = np.diag(main) + np.diag(off,1) + np.diag(off,-1)

b = np.zeros(m)
b[0], b[-1] = r*L, r*R

u = u0.copy()
u_in = u[1:-1].copy()
snapshots = []
snap_times = np.linspace(0, Nt-1, 5, dtype=int)

for j in range(Nt):
    u_in = A @ u_in + b
    u[1:-1] = u_in
    if j in snap_times or j == Nt-1:
        snapshots.append((j*dt, u.copy()))

plt.figure(figsize=(8,4))
first_exact = True
for t_here, u_snap in snapshots:
    plt.plot(x, u_snap, label=f"num t={t_here:.3f}")
    u_exact = np.exp(-np.pi**2*alpha*t_here) * np.sin(np.pi*x)
    if first_exact:
        plt.plot(x, u_exact, 'k--', linewidth=1.2, label='exact solution')
        first_exact = False
    else:
        plt.plot(x, u_exact, 'k--', linewidth=1.2, label="_nolegend_")

plt.xlabel("x"); plt.ylabel("u(x,t)")
plt.title("Heat equation: numerical vs exact")
plt.legend(fontsize="small")
plt.grid(True)
plt.show()
```

---

### Remarks

- The **linear algebra structure** $u^{(j+1)} = A u^{(j)} + b$ makes the scheme compact and systematic.  
- The stability restriction $r \leq \tfrac{1}{2}$ follows from analyzing the eigenvalues of $A$.  
- For large $N_x$, $A$ is sparse and tridiagonal — in practice, use `scipy.sparse.diags` for efficiency.
